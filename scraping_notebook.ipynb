{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM5Ybl7l8uqiVq5YveX1XCL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karalius/Scrape-etsy/blob/master/scraping_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp72u7kPmXOi"
      },
      "source": [
        "# Install dependencies and hide cell output\n",
        "!pip install fake-useragent &> /dev/null\n",
        "# pyopenssl helps to avoid timeouts/bans/recaptcha\n",
        "!pip install pyopenssl &> /dev/null\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxDK9JKiXTaQ"
      },
      "source": [
        "from fake_useragent import UserAgent\n",
        "from sqlalchemy import create_engine\n",
        "from lxml.html import fromstring\n",
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import Set\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import psycopg2\n",
        "import requests\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaKiQYu1h3EP"
      },
      "source": [
        "# Heroku Postgres credentials, define here:\n",
        "DATABASE = \"dfd5ufck729gtm\"\n",
        "USER = \"gkyhzpkfcptiql\"\n",
        "PASSWORD = \"3738f7107f298b27a52a6c3413971c50202d4c9fdea2dfdcd3b8f06c663aac05\"\n",
        "HOST = \"ec2-54-220-35-19.eu-west-1.compute.amazonaws.com\"\n",
        "PORT = \"5432\"\n",
        "sqlalchemy_engine_url = 'postgresql://gkyhzpkfcptiql:3738f7107f298b27a52a6c3413971c50202d4c9fdea2dfdcd3b8f06c663aac05@ec2-54-220-35-19.eu-west-1.compute.amazonaws.com:5432/dfd5ufck729gtm'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLGKqxzKac6f"
      },
      "source": [
        "def get_proxies() -> Set:\n",
        "    url = 'https://www.us-proxy.org/'\n",
        "    response = requests.get(url)\n",
        "    parser = fromstring(response.text)\n",
        "    proxies = set()\n",
        "\n",
        "    # Extract proxy string and add to the set\n",
        "    for i in parser.xpath('//tbody/tr')[:60]:\n",
        "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
        "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
        "            proxies.add(proxy)\n",
        "\n",
        "    # Up to 10 US proxies in the set\n",
        "    return proxies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu_u094bopE9"
      },
      "source": [
        "def replace_attr(html_doc: requests.models.Response, from_attr: str, to_attr: str) -> BeautifulSoup:\n",
        "    soup = BeautifulSoup(html_doc.content, 'html.parser')\n",
        "    tags = soup(attrs={from_attr: True})\n",
        "\n",
        "    # Replace tags with new tag\n",
        "    for tag in tags:\n",
        "        tag[to_attr] = tag[from_attr]\n",
        "        del tag[from_attr]\n",
        "\n",
        "    return soup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCiRA686dJyo"
      },
      "source": [
        "# Available keywords: 'painting', 'photography', 'prints'\n",
        "# Input: up to 3 keywords and items to scrape\n",
        "\n",
        "def scrape_etsy(keywords: list, items_to_scrape: int) -> pd.DataFrame:\n",
        "    average_items_per_page = 40\n",
        "    pages_to_scrape = math.ceil(items_to_scrape / average_items_per_page)\n",
        "    df_list = []\n",
        "\n",
        "    # Define fake user agent\n",
        "    ua = UserAgent(use_cache_server=False)\n",
        "\n",
        "    for key in keywords:\n",
        "        titles, prices, item_urls, img_urls = ([] for i in range(4))\n",
        "        adj_keyword = key.lower()\n",
        "        # Get new set of proxies for each keyword\n",
        "        proxies_set = get_proxies()\n",
        "        \n",
        "        for page_no in range(1, pages_to_scrape + 1):\n",
        "            proxies_to_iter = proxies_set\n",
        "\n",
        "            for proxy in proxies_to_iter.copy():\n",
        "                # Test if proxy works and get page html\n",
        "                try:\n",
        "                    headers = {\n",
        "                        'authority': 'www.etsy.com',\n",
        "                        'sec-ch-ua': '^\\\\^Google',\n",
        "                        'sec-ch-ua-mobile': '?0',\n",
        "                        'upgrade-insecure-requests': '1',\n",
        "                        'user-agent': str(ua.random),\n",
        "                        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "                        'sec-fetch-site': 'same-origin',\n",
        "                        'sec-fetch-mode': 'navigate',\n",
        "                        'sec-fetch-user': '?1',\n",
        "                        'sec-fetch-dest': 'document',\n",
        "                        'referer': f'https://www.etsy.com/c/art-and-collectibles/{adj_keyword}?explicit=1&ship_to=US&ref=pagination&page={page_no-1}',\n",
        "                        'accept-language': 'en-US,en;q=0.9',\n",
        "                    }\n",
        "                    params = (\n",
        "                        ('explicit', '1'),\n",
        "                        ('ref', 'pagination'),\n",
        "                        ('page', f'{page_no}'),\n",
        "                    )\n",
        "                    page = requests.get(\n",
        "                        f\"https://www.etsy.com/c/art-and-collectibles/{adj_keyword}?explicit=1&ship_to=US&ref=pagination&page={page_no}\",\n",
        "                        headers = headers,\n",
        "                        params = params,\n",
        "                        proxies={\"http\": str(proxy), \"https\": str(proxy)}\n",
        "                    )\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    # Remove timed out proxy from the set\n",
        "                    proxies_to_iter.discard(proxy)\n",
        "                    continue\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "            # Replace data-src to src attributes to stay consistent\n",
        "            soup = replace_attr(page,'data-src', 'src')\n",
        "            \n",
        "            for value in soup.find_all('div', class_ = ['js-merch-stash-check-listing']):\n",
        "                # Discard trees with discount price\n",
        "                if value.find(class_='strike-through'):\n",
        "                    value.unwrap()\n",
        "\n",
        "                # Extract and append to lists information on each item\n",
        "                titles.extend([title.get('title') for title in value.find_all('a')])\n",
        "                prices.extend([float(price.get_text().replace(',','')) for price in value.find_all('span', class_='currency-value')])\n",
        "                item_urls.extend([link.get('href') for link in value.find_all('a')])\n",
        "                img_urls.extend([pic.img['src'] for pic in value.find_all('div', class_='height-placeholder')])\n",
        "\n",
        "            #  Sleep anywhere from 0.4s to 1.2s\n",
        "            time.sleep(np.random.uniform(0.4, 1.2))\n",
        "        \n",
        "        # Append dataframes of each keyword and limit length to items_to_scrape\n",
        "        df_list.append(pd.DataFrame({\n",
        "        'category': adj_keyword[:items_to_scrape],\n",
        "        'title': titles[:items_to_scrape],\n",
        "        'price': prices[:items_to_scrape],\n",
        "        'item_url': item_urls[:items_to_scrape],\n",
        "        'img_url': img_urls[:items_to_scrape]\n",
        "    }))\n",
        "   \n",
        "    return pd.concat(df_list, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wom0G51aeEhv"
      },
      "source": [
        "def push_to_heroku(df: pd.DataFrame) -> None:\n",
        "    # Heroku connection\n",
        "    sql_connection = psycopg2.connect(\n",
        "        database=DATABASE,\n",
        "        user=USER,\n",
        "        password=PASSWORD,\n",
        "        host=HOST,\n",
        "        port=PORT\n",
        "    )\n",
        "    # Connect to DB\n",
        "    cur = sql_connection.cursor()\n",
        "    \n",
        "    # Create two tables\n",
        "    cur.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS categories (\n",
        "        id serial PRIMARY KEY,\n",
        "        category varchar(250)\n",
        "    );\n",
        "\n",
        "    CREATE TABLE IF NOT EXISTS items (\n",
        "        id serial PRIMARY KEY,\n",
        "        category_id int,\n",
        "        title varchar(250),\n",
        "        price float(2),\n",
        "        item_url varchar(500),\n",
        "        img_url varchar(500),\n",
        "        FOREIGN KEY (category_id) REFERENCES categories(id)\n",
        "    );\n",
        "\n",
        "    ''')\n",
        "    \n",
        "    #Get array of unique category names\n",
        "    unique_categories = df['category'].unique()\n",
        "\n",
        "    # Insert unique category names to the categories table\n",
        "    for i in unique_categories:\n",
        "        cur.execute(f\"INSERT INTO categories (category) VALUES ('{i}');\")\n",
        "\n",
        "    sql_connection.commit()\n",
        "\n",
        "    sql_connection.close()\n",
        "\n",
        "    # Create category id for each category\n",
        "    foreign_key_df = pd.DataFrame(df['category'].unique(), columns=['category']).reset_index().rename(columns={'index': 'category_id'})\n",
        "    foreign_key_df['category_id'] = np.arange(1, len(foreign_key_df)+1)\n",
        "\n",
        "    # Make df only with category id\n",
        "    items_df = pd.merge(\n",
        "        df,\n",
        "        foreign_key_df,\n",
        "        on='category'\n",
        "    ).drop(columns='category')\n",
        "\n",
        "    # Put category id as first column\n",
        "    items_df.insert(0, 'category_id', items_df.pop('category_id'))\n",
        "\n",
        "    # Connect to Heroku Postgres\n",
        "    conn = create_engine(sqlalchemy_engine_url)\n",
        "\n",
        "    # Push items df to Heroku DB\n",
        "    items_df.to_sql('items', conn, method='multi', if_exists='append', chunksize=10000, index=False)\n",
        "\n",
        "    conn.dispose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RevC0MQWex9Q"
      },
      "source": [
        "def get_csv() -> None:\n",
        "    # Heroku connection\n",
        "    sql_connection = psycopg2.connect(\n",
        "        database=DATABASE,\n",
        "        user=USER,\n",
        "        password=PASSWORD,\n",
        "        host=HOST,\n",
        "        port=PORT\n",
        "    )\n",
        "    # Connect to DB\n",
        "    cur = sql_connection.cursor()\n",
        "\n",
        "    # Join tables on category id\n",
        "    s = \"SELECT items.id, categories.category, items.title, items.price, items.item_url, items.img_url FROM items JOIN categories ON categories.id = items.category_id ORDER BY id ASC\"\n",
        "\n",
        "    # COPY function on the SQL we created above.\n",
        "    SQL_for_file_output = \"COPY ({0}) TO STDOUT WITH CSV HEADER\".format(s)\n",
        "\n",
        "    # Csv file\n",
        "    with open('etsy_data.csv', 'w') as f_output:\n",
        "        cur.copy_expert(SQL_for_file_output, f_output)\n",
        "\n",
        "    files.download('etsy_data.csv')\n",
        "\n",
        "    sql_connection.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIvG76ZnR0MO"
      },
      "source": [
        "# Call scrape_etsy function and save it as variable\n",
        "df = scrape_etsy(['painting', 'photography', 'prints'], 3000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8D2UP_-h5p9"
      },
      "source": [
        "push_to_heroku()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjpwN6AXh8Dc"
      },
      "source": [
        "get_csv()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}