{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORlwAaIociR7uQja/w5AyD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karalius/etsy-scraper/blob/master/scraping_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp72u7kPmXOi"
      },
      "source": [
        "# Install dependencies and hide cell output\n",
        "!pip install fake-useragent &> /dev/null\n",
        "# pyopenssl helps to avoid timeouts/bans/recaptcha\n",
        "!pip install pyopenssl &> /dev/null\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxDK9JKiXTaQ"
      },
      "source": [
        "from fake_useragent import UserAgent\n",
        "from sqlalchemy import create_engine\n",
        "from lxml.html import fromstring\n",
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import Set\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import psycopg2\n",
        "import requests\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heroku Postgres credentials, define here:"
      ],
      "metadata": {
        "id": "qcTRT_TtegSp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaKiQYu1h3EP"
      },
      "source": [
        "DATABASE = \"\"\n",
        "USER = \"\"\n",
        "PASSWORD = \"\"\n",
        "HOST = \"\"\n",
        "PORT = \"\"\n",
        "sqlalchemy_engine_url = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLGKqxzKac6f"
      },
      "source": [
        "def get_proxies() -> Set:\n",
        "    \"\"\"\n",
        "    This function fetches html from the www.us-proxy.org website and iterates through it.\n",
        "    While iterating it collects IP addresses and ports, then joins them to form a string.\n",
        "    :return: a set of strings containing proxies based in the US.\"\"\"\n",
        "\n",
        "    url = \"https://www.us-proxy.org/\"\n",
        "    response = requests.get(url)\n",
        "    parser = fromstring(response.text)\n",
        "    proxies = set()\n",
        "\n",
        "    # Extract proxy string and add to the set\n",
        "    for i in parser.xpath(\"//tbody/tr\")[:60]:\n",
        "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
        "            proxy = \":\".join(\n",
        "                [i.xpath(\".//td[1]/text()\")[0], i.xpath(\".//td[2]/text()\")[0]]\n",
        "            )\n",
        "            proxies.add(proxy)\n",
        "\n",
        "    return proxies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu_u094bopE9"
      },
      "source": [
        "def replace_attr(\n",
        "    html_doc: requests.models.Response, from_attr: str, to_attr: str\n",
        ") -> BeautifulSoup:\n",
        "    \"\"\"\n",
        "    Function takes the arguments explained below and returns edited html page with new tags\n",
        "    :param html_doc: takes a response page, for example: requests.get(url)\n",
        "    :param from_attr: give a tag name which meant to be changed, for example: 'data-src'\n",
        "    :param to_attr: give a tag name which should replace from_attr, for example: 'src'\n",
        "    :return: returns response page with new tag name in the html\n",
        "    \"\"\"\n",
        "\n",
        "    soup = BeautifulSoup(html_doc.content, \"html.parser\")\n",
        "    tags = soup(attrs={from_attr: True})\n",
        "\n",
        "    # Replace tags with new tag\n",
        "    for tag in tags:\n",
        "        tag[to_attr] = tag[from_attr]\n",
        "        del tag[from_attr]\n",
        "\n",
        "    return soup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Available keywords:\n",
        "\n",
        "prints, drawing-and-illustration, painting, collectibles, photography, sculpture, dolls-and-miniatures, fiber-arts, fine-art-ceramics, mixed-media-and-college, glass-art, artist-tradings-cards"
      ],
      "metadata": {
        "id": "NbY2bOrhe476"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCiRA686dJyo"
      },
      "source": [
        "def scrape_etsy(keywords: list, items_to_scrape: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This function scrapes https://www.etsy.com/c/art-and-collectibles for the given number of items for each of keyword.\n",
        "    It features rotating user-agents mirroring a real agent, also rotating US proxies to avoid suspicion (bans).\n",
        "    This function returns pandas dataframe with the collected information:\n",
        "    Category (keyword), title, price, item url, image url.\n",
        "    :param keywords: list of keywords to scrape.\n",
        "    Available keywords: painting, photography, prints.\n",
        "    Constrain: up to 12 keywords in the list! Check the website for the names.\n",
        "    Example, keyword on the web: Drawing & Illustration -> ['drawing-and-illustration'] for the scraper.\n",
        "    :param items_to_scrape: integer of items to scrape for each keyword.\n",
        "    For example: 3000 - every keyword in the keywords list will be scraped for 3000 items.\n",
        "    :return: pandas dataframe with the following columns:\n",
        "    Category (keyword), title, price, item url, image url.\n",
        "    \"\"\"\n",
        "    average_items_per_page = 40\n",
        "    pages_to_scrape = math.ceil(items_to_scrape / average_items_per_page)\n",
        "    df_list = []\n",
        "\n",
        "    ua = UserAgent(use_cache_server=False)\n",
        "\n",
        "    for key in keywords:\n",
        "        titles, prices, item_urls, img_urls = ([] for i in range(4))\n",
        "        adj_keyword = key.lower()\n",
        "        proxies_set = get_proxies()\n",
        "\n",
        "        for page_no in range(1, pages_to_scrape + 1):\n",
        "            proxies_to_iter = proxies_set\n",
        "\n",
        "            for proxy in proxies_to_iter.copy():\n",
        "                try:\n",
        "                    headers = {\n",
        "                        \"authority\": \"www.etsy.com\",\n",
        "                        \"sec-ch-ua\": \"^\\\\^Google\",\n",
        "                        \"sec-ch-ua-mobile\": \"?0\",\n",
        "                        \"upgrade-insecure-requests\": \"1\",\n",
        "                        \"user-agent\": str(ua.random),\n",
        "                        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
        "                        \"sec-fetch-site\": \"same-origin\",\n",
        "                        \"sec-fetch-mode\": \"navigate\",\n",
        "                        \"sec-fetch-user\": \"?1\",\n",
        "                        \"sec-fetch-dest\": \"document\",\n",
        "                        \"referer\": f\"https://www.etsy.com/c/art-and-collectibles/{adj_keyword}?explicit=1&ship_to=US&ref=pagination&page={page_no-1}\",\n",
        "                        \"accept-language\": \"en-US,en;q=0.9\",\n",
        "                    }\n",
        "                    params = (\n",
        "                        (\"explicit\", \"1\"),\n",
        "                        (\"ref\", \"pagination\"),\n",
        "                        (\"page\", f\"{page_no}\"),\n",
        "                    )\n",
        "                    page = requests.get(\n",
        "                        f\"https://www.etsy.com/c/art-and-collectibles/{adj_keyword}?explicit=1&ship_to=US&ref=pagination&page={page_no}\",\n",
        "                        headers=headers,\n",
        "                        params=params,\n",
        "                        proxies={\"http\": str(proxy), \"https\": str(proxy)},\n",
        "                    )\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    proxies_to_iter.discard(proxy)\n",
        "                    continue\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "            soup = replace_attr(page, \"data-src\", \"src\")\n",
        "\n",
        "            for value in soup.find_all(\"div\", class_=[\"js-merch-stash-check-listing\"]):\n",
        "                if value.find(class_=\"strike-through\"):\n",
        "                    value.unwrap()\n",
        "\n",
        "                titles.extend([title.get(\"title\") for title in value.find_all(\"a\")])\n",
        "                prices.extend(\n",
        "                    [\n",
        "                        float(price.get_text().replace(\",\", \"\"))\n",
        "                        for price in value.find_all(\"span\", class_=\"currency-value\")\n",
        "                    ]\n",
        "                )\n",
        "                item_urls.extend([link.get(\"href\") for link in value.find_all(\"a\")])\n",
        "                img_urls.extend(\n",
        "                    [\n",
        "                        pic.img[\"src\"]\n",
        "                        for pic in value.find_all(\"div\", class_=\"height-placeholder\")\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "            time.sleep(np.random.uniform(0.4, 1.2))\n",
        "\n",
        "        df_list.append(\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    \"category\": adj_keyword[:items_to_scrape],\n",
        "                    \"title\": titles[:items_to_scrape],\n",
        "                    \"price\": prices[:items_to_scrape],\n",
        "                    \"item_url\": item_urls[:items_to_scrape],\n",
        "                    \"img_url\": img_urls[:items_to_scrape],\n",
        "                }\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return pd.concat(df_list, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wom0G51aeEhv"
      },
      "source": [
        "def push_to_heroku(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Functions takes a dataframe as an argument, then connects to a Heroku Postgres database.\n",
        "    In the database it creates two tables pushes the information into both of them.\n",
        "    :param df: Takes a pandas dataframe.\n",
        "    :return: None, data sits on Heroku Postgres database.\n",
        "    \"\"\"\n",
        "    sql_connection = psycopg2.connect(\n",
        "        database=DATABASE, user=USER, password=PASSWORD, host=HOST, port=PORT\n",
        "    )\n",
        "    cur = sql_connection.cursor()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS categories (\n",
        "        id serial PRIMARY KEY,\n",
        "        category varchar(250)\n",
        "    );\n",
        "\n",
        "    CREATE TABLE IF NOT EXISTS items (\n",
        "        id serial PRIMARY KEY,\n",
        "        category_id int,\n",
        "        title varchar(250),\n",
        "        price float(2),\n",
        "        item_url varchar(500),\n",
        "        img_url varchar(500),\n",
        "        FOREIGN KEY (category_id) REFERENCES categories(id)\n",
        "    );\n",
        "\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    unique_categories = df[\"category\"].unique()\n",
        "\n",
        "    for i in unique_categories:\n",
        "        cur.execute(f\"INSERT INTO categories (category) VALUES ('{i}');\")\n",
        "\n",
        "    sql_connection.commit()\n",
        "    sql_connection.close()\n",
        "\n",
        "    foreign_key_df = (\n",
        "        pd.DataFrame(df[\"category\"].unique(), columns=[\"category\"])\n",
        "        .reset_index()\n",
        "        .rename(columns={\"index\": \"category_id\"})\n",
        "    )\n",
        "    foreign_key_df[\"category_id\"] = np.arange(1, len(foreign_key_df) + 1)\n",
        "\n",
        "    items_df = pd.merge(df, foreign_key_df, on=\"category\").drop(columns=\"category\")\n",
        "\n",
        "    items_df.insert(0, \"category_id\", items_df.pop(\"category_id\"))\n",
        "    conn = create_engine(sqlalchemy_engine_url)\n",
        "    items_df.to_sql(\n",
        "        \"items\", conn, method=\"multi\", if_exists=\"append\", chunksize=10000, index=False\n",
        "    )\n",
        "    conn.dispose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RevC0MQWex9Q"
      },
      "source": [
        "def get_csv() -> None:\n",
        "    \"\"\"\n",
        "    This functions takes a path as an argument as saves the etsy_data.csv file in provided local directory.\n",
        "    :param path: Provide a path where to save a csv file.\n",
        "    :return: None, csv file saved in the local directory.\n",
        "    \"\"\"\n",
        "    sql_connection = psycopg2.connect(\n",
        "        database=DATABASE, user=USER, password=PASSWORD, host=HOST, port=PORT\n",
        "    )\n",
        "    cur = sql_connection.cursor()\n",
        "\n",
        "    s = \"SELECT items.id, categories.category, items.title, items.price, items.item_url, items.img_url FROM items JOIN categories ON categories.id = items.category_id ORDER BY id ASC\"\n",
        "\n",
        "    SQL_for_file_output = \"COPY ({0}) TO STDOUT WITH CSV HEADER\".format(s)\n",
        "\n",
        "    with open(\"etsy_data.csv\", \"w\") as f_output:\n",
        "        cur.copy_expert(SQL_for_file_output, f_output)\n",
        "    files.download(\"etsy_data.csv\")\n",
        "    sql_connection.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:"
      ],
      "metadata": {
        "id": "TngTOC58fmsr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIvG76ZnR0MO"
      },
      "source": [
        "df = scrape_etsy(\n",
        "    [\"dolls-and-miniatures\", \"photography\", \"prints\", \"mixed-media-and-collage\"], 100\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8D2UP_-h5p9"
      },
      "source": [
        "push_to_heroku()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjpwN6AXh8Dc"
      },
      "source": [
        "get_csv()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}